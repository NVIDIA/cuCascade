{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cuCascade Benchmark Results Visualization\n",
    "\n",
    "This notebook visualizes benchmark results from Google Benchmark output (gbench_results.json) using Plotly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Benchmark Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "json_file = 'gbench_results.json'\n",
    "\n",
    "with open(json_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract benchmarks\n",
    "benchmarks = data['benchmarks']\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(benchmarks)\n",
    "\n",
    "print(f\"Loaded {len(df)} benchmark results\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract benchmark name components\n",
    "def parse_benchmark_name(name):\n",
    "    \"\"\"Parse benchmark name to extract test name and parameters.\"\"\"\n",
    "    parts = name.split('/')\n",
    "    base_name = parts[0]\n",
    "    params = parts[1:] if len(parts) > 1 else []\n",
    "    return base_name, params\n",
    "\n",
    "df['base_name'], df['params'] = zip(*df['name'].apply(parse_benchmark_name))\n",
    "\n",
    "# Extract thread count from name (Google Benchmark format: benchmark_name/threads:N)\n",
    "df['threads'] = df['threads'] if 'threads' in df.columns else 1\n",
    "\n",
    "# Calculate throughput in GB/s\n",
    "if 'bytes_per_second' in df.columns:\n",
    "    df['throughput_GBs'] = df['bytes_per_second'] / (1024**3)\n",
    "\n",
    "# Convert time to milliseconds if needed\n",
    "if 'real_time' in df.columns:\n",
    "    df['time_ms'] = df['real_time'] if df['time_unit'].iloc[0] == 'ms' else df['real_time'] / 1000\n",
    "\n",
    "print(f\"\\nUnique benchmark types: {df['base_name'].unique()}\")\n",
    "print(f\"Thread counts: {sorted(df['threads'].unique())}\")\n",
    "df[['name', 'base_name', 'threads', 'throughput_GBs', 'time_ms']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Conversion Benchmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and process different benchmark types\n",
    "throughput_benchmarks = df[df['base_name'].str.contains('Throughput', na=False)].copy()\n",
    "conversion_benchmarks = df[\n",
    "    df['base_name'].str.contains('Convert', na=False) & \n",
    "    ~df['base_name'].str.contains('Throughput', na=False)\n",
    "].copy()\n",
    "\n",
    "# Process throughput benchmarks\n",
    "if not throughput_benchmarks.empty and 'MB' in throughput_benchmarks.columns:\n",
    "    throughput_benchmarks['size_MB'] = throughput_benchmarks['MB']\n",
    "\n",
    "# Process conversion benchmarks\n",
    "if not conversion_benchmarks.empty:\n",
    "    # Extract rows and columns counters\n",
    "    if 'columns' in conversion_benchmarks.columns:\n",
    "        conversion_benchmarks['num_columns'] = conversion_benchmarks['columns']\n",
    "    elif 'param1' in conversion_benchmarks.columns:\n",
    "        conversion_benchmarks['num_columns'] = conversion_benchmarks['param1']\n",
    "    \n",
    "    # Calculate data size in MB\n",
    "    if 'bytes' in conversion_benchmarks.columns:\n",
    "        conversion_benchmarks['size_MB'] = conversion_benchmarks['bytes'] / (1024 * 1024)\n",
    "    elif 'param0' in conversion_benchmarks.columns:\n",
    "        # param0 is total bytes\n",
    "        conversion_benchmarks['size_MB'] = conversion_benchmarks['param0'] / (1024 * 1024)\n",
    "\n",
    "print(f\"Throughput benchmarks: {len(throughput_benchmarks)}\")\n",
    "print(f\"Conversion benchmarks: {len(conversion_benchmarks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis by Thread Count\n",
    "\n",
    "Create separate plots for each thread count. Each plot shows HostToGpu (left) and GpuToHost (right) with both raw and conversion results combined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique thread counts\n",
    "thread_counts = sorted(df['threads'].unique())\n",
    "\n",
    "print(f\"Creating plots for thread counts: {thread_counts}\")\n",
    "\n",
    "# Filter for GpuToHost and HostToGpu benchmarks\n",
    "filtered_throughput = throughput_benchmarks[\n",
    "    throughput_benchmarks['base_name'].str.contains('GpuToHost|HostToGpu', na=False)\n",
    "].copy()\n",
    "\n",
    "filtered_conversion = conversion_benchmarks[\n",
    "    conversion_benchmarks['base_name'].str.contains('GpuToHost|HostToGpu', na=False)\n",
    "].copy()\n",
    "\n",
    "# Create separate plot for each thread count\n",
    "for thread_count in thread_counts:\n",
    "    # Filter data for this thread count\n",
    "    thread_throughput = filtered_throughput[filtered_throughput['threads'] == thread_count]\n",
    "    thread_conversion = filtered_conversion[filtered_conversion['threads'] == thread_count]\n",
    "    \n",
    "    # Create subplot with 1 row and 2 columns (HostToGpu left, GpuToHost right)\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=(f'HostToGpu', f'GpuToHost'),\n",
    "        horizontal_spacing=0.12,\n",
    "        shared_yaxes=True\n",
    "    )\n",
    "    \n",
    "    # Collect all throughput values to determine y-axis range\n",
    "    all_throughputs = []\n",
    "    \n",
    "    marker_symbols = ['circle', 'square', 'diamond', 'cross', 'x', 'triangle-up', \n",
    "                      'triangle-down', 'star', 'hexagon', 'pentagon', 'octagon']\n",
    "    colors_raw = px.colors.qualitative.Set1\n",
    "    colors_conv = px.colors.qualitative.Plotly\n",
    "    \n",
    "    # === Left column: HostToGpu (raw + conversion) ===\n",
    "    \n",
    "    # Add raw HostToGpu throughput\n",
    "    h2g_raw = thread_throughput[thread_throughput['base_name'].str.contains('HostToGpu', na=False)]\n",
    "    if not h2g_raw.empty:\n",
    "        h2g_raw_sorted = h2g_raw.sort_values('size_MB')\n",
    "        all_throughputs.extend(h2g_raw_sorted['throughput_GBs'].tolist())\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=h2g_raw_sorted['size_MB'],\n",
    "                y=h2g_raw_sorted['throughput_GBs'],\n",
    "                mode='lines+markers',\n",
    "                name='Raw',\n",
    "                line=dict(width=3, color=colors_raw[1]),\n",
    "                marker=dict(size=10),\n",
    "                legendgroup='h2g',\n",
    "                legendgrouptitle_text='HostToGpu',\n",
    "                showlegend=True\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Add HostToGpu conversion benchmarks\n",
    "    h2g_conv = thread_conversion[thread_conversion['base_name'].str.contains('HostToGpu', na=False)]\n",
    "    if not h2g_conv.empty and 'size_MB' in h2g_conv.columns and 'num_columns' in h2g_conv.columns:\n",
    "        unique_cols = sorted(h2g_conv['num_columns'].unique())\n",
    "        \n",
    "        for col_idx, num_cols in enumerate(unique_cols):\n",
    "            col_data = h2g_conv[h2g_conv['num_columns'] == num_cols].sort_values('size_MB')\n",
    "            all_throughputs.extend(col_data['throughput_GBs'].tolist())\n",
    "            \n",
    "            trace_name = f\"Convert ({int(num_cols)}col)\"\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=col_data['size_MB'],\n",
    "                    y=col_data['throughput_GBs'],\n",
    "                    mode='markers+lines',\n",
    "                    name=trace_name,\n",
    "                    marker=dict(\n",
    "                        size=8,\n",
    "                        symbol=marker_symbols[col_idx % len(marker_symbols)],\n",
    "                        color=colors_conv[col_idx % len(colors_conv)],\n",
    "                        line=dict(width=1, color='white')\n",
    "                    ),\n",
    "                    line=dict(width=1.5),\n",
    "                    legendgroup='h2g',\n",
    "                    showlegend=True\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "    \n",
    "    # === Right column: GpuToHost (raw + conversion) ===\n",
    "    \n",
    "    # Add raw GpuToHost throughput\n",
    "    g2h_raw = thread_throughput[thread_throughput['base_name'].str.contains('GpuToHost', na=False)]\n",
    "    if not g2h_raw.empty:\n",
    "        g2h_raw_sorted = g2h_raw.sort_values('size_MB')\n",
    "        all_throughputs.extend(g2h_raw_sorted['throughput_GBs'].tolist())\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=g2h_raw_sorted['size_MB'],\n",
    "                y=g2h_raw_sorted['throughput_GBs'],\n",
    "                mode='lines+markers',\n",
    "                name='Raw',\n",
    "                line=dict(width=3, color=colors_raw[0]),\n",
    "                marker=dict(size=10),\n",
    "                legendgroup='g2h',\n",
    "                legendgrouptitle_text='GpuToHost',\n",
    "                showlegend=True\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # Add GpuToHost conversion benchmarks\n",
    "    g2h_conv = thread_conversion[thread_conversion['base_name'].str.contains('GpuToHost', na=False)]\n",
    "    if not g2h_conv.empty and 'size_MB' in g2h_conv.columns and 'num_columns' in g2h_conv.columns:\n",
    "        unique_cols = sorted(g2h_conv['num_columns'].unique())\n",
    "        \n",
    "        for col_idx, num_cols in enumerate(unique_cols):\n",
    "            col_data = g2h_conv[g2h_conv['num_columns'] == num_cols].sort_values('size_MB')\n",
    "            all_throughputs.extend(col_data['throughput_GBs'].tolist())\n",
    "            \n",
    "            trace_name = f\"Convert ({int(num_cols)}col)\"\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=col_data['size_MB'],\n",
    "                    y=col_data['throughput_GBs'],\n",
    "                    mode='markers+lines',\n",
    "                    name=trace_name,\n",
    "                    marker=dict(\n",
    "                        size=8,\n",
    "                        symbol=marker_symbols[col_idx % len(marker_symbols)],\n",
    "                        color=colors_conv[col_idx % len(colors_conv)],\n",
    "                        line=dict(width=1, color='white')\n",
    "                    ),\n",
    "                    line=dict(width=1.5),\n",
    "                    legendgroup='g2h',\n",
    "                    showlegend=True\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "    \n",
    "    # Calculate y-axis range with some padding\n",
    "    if all_throughputs:\n",
    "        y_min = min(all_throughputs) * 0.9\n",
    "        y_max = max(all_throughputs) * 1.1\n",
    "    else:\n",
    "        y_min, y_max = 0, 100\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text=\"Data Size (MB)\", type=\"log\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Data Size (MB)\", type=\"log\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Throughput (GB/s)\", range=[y_min, y_max], row=1, col=1)\n",
    "    fig.update_yaxes(range=[y_min, y_max], row=1, col=2)\n",
    "    \n",
    "    # Update layout\n",
    "    thread_label = f\"{thread_count} thread\" if thread_count == 1 else f\"{thread_count} threads\"\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Performance with {thread_label}: HostToGpu vs GpuToHost\",\n",
    "        template='plotly_white',\n",
    "        width=1600,\n",
    "        height=600,\n",
    "        hovermode='closest',\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=0.98,\n",
    "            xanchor=\"left\",\n",
    "            x=1.01,\n",
    "            font=dict(size=10),\n",
    "            tracegroupgap=20\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
